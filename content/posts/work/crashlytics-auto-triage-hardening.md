---
title: "Crashlytics 자동 대응 고도화 기록"
description: "과금 제약이 있는 환경에서 Slack Events + n8n + BigQuery 폴링으로 운영 가능한 구조를 만든 기록"
summary: "Slack Events 중심 재구성, BigQuery 폴링 보강, 재부팅 복구 스킬 표준화로 자동 대응 운영성을 높인 과정"
date: 2026-02-20
startDate: 2026-02-20
endDate: 2026-02-20
contribution: 1.0
showDate: true
showAuthor: false
draft: true
categories: ["work"]
tags: ["android", "crashlytics", "slack", "n8n", "bigquery", "automation", "incident-response", "operations"]
series: ["Elecvery Contributions"]
---

### 과금 제약이 있는 환경에서 Slack Events + n8n + BigQuery 폴링으로 운영 가능한 구조 만들기

기존에 작성한 글([Crashlytics Slack 링크 기반 장애 트리아지 자동화](https://sbeenx.io/posts/work/crashlytics-slack-triage/))이 "트리아지 자동화의 시작"이었다면,  
이번 작업은 실제 운영에서 끊기지 않게 만드는 데 초점을 맞췄습니다.

핵심은 세 가지였습니다.

1. 문제를 다시 정의하고
2. 제약(과금/운영환경) 안에서 동작하는 방식으로 바꾸고
3. 재부팅이나 담당자 변경이 있어도 다시 살릴 수 있게 문서와 스킬까지 묶는 것

---

## 문제 정의: 왜 자동화가 있어도 대응이 끊겼는가

초기 목표는 분명했습니다.  
Crashlytics 이슈가 발생하면 Slack에서 감지하고, 분석해서, GitHub 이슈와 수정까지 자동으로 이어지게 만들고 싶었습니다.

그런데 실제로 돌려보니 병목은 다른 데 있었습니다.

1. Slack 본문 정보가 부족했습니다.  
   `Summary: com.xxx.SomeFragment.onCreateView`처럼 위치 정보 위주라, 원인 근거를 바로 만들기 어려웠습니다.
2. 운영 경로가 길었습니다.  
   알림 확인, 원인 추정, 댓글 작성, 이슈 생성, 브랜치 생성이 사람 손을 거치며 끊겼습니다.
3. 회사 환경상 과금 의사결정이 민감했습니다.  
   좋은 구조라도 결제 계정이 필요한 순간, 즉시 도입이 어려웠습니다.

즉, 문제는 "분석 모델 성능"보다  
분석 이후 액션까지 한 흐름으로 이어지지 않는 운영 구조에 있었습니다.

---

## 용어를 먼저 짧게 정리하겠습니다

처음 보는 분도 따라오실 수 있도록 핵심 용어만 간단히 정리합니다.

- Crashlytics: 앱 크래시를 수집하고 이슈로 묶어주는 Firebase 기능
- Webhook: 이벤트가 발생하면 특정 URL로 HTTP 요청을 보내는 방식
- n8n: 여러 API를 연결해 자동화 흐름을 만드는 워크플로우 도구
- Slack Event Subscriptions: Slack 메시지 이벤트를 서버로 전달하는 기능
- BigQuery: 대용량 로그/데이터를 SQL로 조회하는 분석 서비스
- 폴링(Polling): 데이터가 들어왔는지 일정 간격으로 반복 조회하는 방식

---

## 해결 과정

### 1) Google Cloud Webhook 중심 설계에서 Slack Events 중심으로 전환

처음에는 Google Cloud 쪽 Webhook 경로를 중심으로 잡았습니다.  
하지만 설정 단계에서 결제 계정이 필요한 지점이 확인됐고, 회사 상황을 고려하면 핵심 트리거로 채택하기 어려웠습니다.

그래서 방향을 바꿨습니다.

- 핵심 트리거는 이미 쓰고 있는 Slack 알림 이벤트로 고정
- Google Cloud 경로는 필수 의존성에서 제거

이 결정 덕분에 "도입 가능성"이 크게 올라갔습니다.

---

### 2) MCP Bridge 의존도를 낮추고 n8n에서 직접 API 호출

운영 복잡도를 줄이기 위해 브리지 계층을 줄였습니다.  
n8n에서 아래 API를 직접 호출하도록 재구성했습니다.

- OpenAI API
- Slack API
- GitHub API

결과적으로 로컬에서 별도 브리지를 유지해야 하는 부담이 줄었고,  
문제 발생 시 어느 단계에서 실패했는지 추적하기 쉬워졌습니다.

---

### 3) Slack 스레드 중심으로 결과를 남기도록 고정

분석 결과를 새 메시지로 흩뿌리면, 나중에 맥락이 사라집니다.  
그래서 같은 이슈는 반드시 같은 스레드에서 이어지게 만들었습니다.

댓글 구조도 고정했습니다.

1. 요약
2. 영향 범위
3. 원인(가설/확정)
4. 근거
5. 권장 액션

여기까지 맞추니 "누가 어디까지 확인했는지"가 스레드만 봐도 보였습니다.

---

### 4) BigQuery 폴링으로 정보 부족 문제 보강

Slack 본문만으로는 근거가 부족했기 때문에, BigQuery를 보강 소스로 붙였습니다.  
다만 실시간 스트리밍 의존이 아니라 폴링 방식으로 구성했습니다.

기본 동작은 다음과 같습니다.

- 최초 대기: 90초
- 폴링 주기: 180초
- 최대 시도: 6회

그리고 중요한 운영 포인트를 하나 넣었습니다.  
각 시도 결과를 Slack 스레드에 그대로 남기도록 했습니다.

- 조회 시작
- 데이터 없음, N초 후 재시도
- 조회 성공
- 최종 미수집(기본 페이로드로 계속 진행)

즉, 자동화가 백그라운드에서 조용히 실패하지 않게 했습니다.

---

### 5) 재부팅 복구를 스킬로 표준화

실무에서 자주 터지는 문제는 재부팅 이후였습니다.

- Docker 내려감
- 터널 URL 바뀜
- Slack Request URL 불일치
- 웹훅 수신 중단

이걸 매번 기억으로 복구하면 품질이 흔들립니다.  
그래서 `n8n-reboot-recovery` 스킬로 절차를 고정했습니다.

스킬이 하는 일:

- Docker/n8n 재기동
- health check
- Cloudflare quick tunnel 재생성
- 현재 웹훅 URL 재계산
- `.env` placeholder 키 점검
- BigQuery 준비 상태(`BIGQUERY_READY`, `BIGQUERY_MISSING_KEYS`) 점검
- 사람이 해야 할 일(`MANUAL_1~4`) 안내

"자동화 + 수동 후속 작업 안내"까지 포함해서, 실제 운영 복구 시간을 줄였습니다.

---

### 6) 문서와 스킬을 레포에 함께 포함

담당자가 바뀌거나 다른 PC로 이식될 때가 가장 위험합니다.  
그래서 스크립트만 두지 않고 문서/스킬까지 레포에 같이 넣었습니다.

- `README.md`: 빠른 시작 + 핵심 개념
- `docs/operations/slack-events-runbook.md`: 운영/장애 대응
- `skills/n8n-reboot-recovery`: 복구 스킬 본체
- `scripts/install_skills.sh`: 스킬 설치 자동화

결과적으로 "어떻게 세팅했는지"를 사람 기억에 의존하지 않게 됐습니다.

---

## 적용 후 기대 효과

1. 대응 리드타임 단축  
   이슈 감지 이후 분석/댓글/이슈/브랜치/수정까지 한 흐름으로 이어집니다.
2. 스레드 기반 가시성 확보  
   현재 단계(분석중/폴링중/성공/실패)를 팀원이 같은 화면에서 확인할 수 있습니다.
3. 과금 리스크 제어  
   핵심 트리거를 과금 민감 경로에서 분리해, 운영 지속 가능성이 높아졌습니다.
4. 이식성과 복구력 향상  
   다른 PC에서도 스킬 실행으로 동일한 절차를 재현할 수 있습니다.

---

## 실제로 겪었던 트러블슈팅 요약

1. 웹훅 URL 공백 문제  
   URL 중간 공백 때문에 수신이 안 됐고, 공백 제거 후 즉시 정상화됐습니다.
2. 재부팅 후 Request URL 불일치  
   터널 URL이 바뀌는 특성 때문에 Slack 검증 실패가 발생했고, 복구 스킬로 재생성/재등록하도록 표준화했습니다.
3. BigQuery 데이터 지연  
   즉시 데이터가 없을 수 있어 "재시도 상태 댓글"을 넣어 운영 불확실성을 줄였습니다.
4. 작성자/환경 편차  
   Git author, env 누락 같은 운영 편차를 문서/스킬에서 강제 확인하도록 보강했습니다.

---

## 마무리

이번 고도화에서 가장 중요했던 건 "기술적으로 가장 화려한 구조"가 아니었습니다.  
우리 조직 제약 안에서 실제로 돌아가고, 장애가 나도 다시 살릴 수 있는 구조를 만드는 것이 핵심이었습니다.

정리하면 이렇게 볼 수 있습니다.

- 문제정의: 분석 자체보다, 분석 이후 흐름이 끊기는 것이 본질
- 해결과정: Slack Events 중심 재구성 + BigQuery 폴링 보강 + 복구 스킬 표준화
- 기대효과: 대응 속도, 가시성, 운영 안정성, 이식성 개선

다음 단계는 명확합니다.  
앱에서 수집하는 Crash 컨텍스트 품질(커스텀 키/로그)을 높이고,  
현재 자동화 흐름에 그 신호를 더 정확히 연결하는 작업입니다.
